{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from IPython.display import Video\n",
    "from tetris_gymnasium.envs.tetris import Tetris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b96a3",
   "metadata": {},
   "source": [
    "### **Wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3ea08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class TetrisObsWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Define final observation space (4 channels, 24x18)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(4, 24, 18),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        board = obs[\"board\"].astype(np.float32) / 9.0\n",
    "        mask = obs[\"active_tetromino_mask\"].astype(np.float32)\n",
    "        holder = obs[\"holder\"].astype(np.float32) / 9.0\n",
    "        queue = obs[\"queue\"].astype(np.float32) / 9.0\n",
    "\n",
    "        # pad holder to (24, 18)\n",
    "        holder_padded = np.zeros((24, 18), dtype=np.float32)\n",
    "        holder_padded[:holder.shape[0], :holder.shape[1]] = holder\n",
    "\n",
    "        # pad or crop queue safely to fit (24, 18)\n",
    "        queue_padded = np.zeros((24, 18), dtype=np.float32)\n",
    "        h, w = queue.shape\n",
    "        queue_padded[:min(4, h), :min(18, w)] = queue[:min(4, h), :min(18, w)]\n",
    "\n",
    "        # stack all channels\n",
    "        stacked = np.stack([board, mask, holder_padded, queue_padded], axis=0)\n",
    "        return stacked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4bdee",
   "metadata": {},
   "source": [
    "### **Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6889c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        C, H, W = obs_shape  # e.g. (4, 24, 18)\n",
    "\n",
    "        # Safer kernel/stride combo for small inputs\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(C, 32, kernel_size=3, stride=1, padding=1),  # keeps 24x18\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # halves size\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),  # halves again\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Automatically compute flattened conv output size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, C, H, W)\n",
    "            conv_out = self.conv(dummy)\n",
    "            conv_out_size = conv_out.view(1, -1).size(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57040f2e",
   "metadata": {},
   "source": [
    "### **ReplayBuffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "625b377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.device = device  # 'cuda' or 'cpu'\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Store raw (numpy or list) to save memory instead of storing tensors directly\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and pin memory for faster GPU transfer\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).pin_memory()\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).pin_memory()\n",
    "        actions = torch.tensor(actions, dtype=torch.long).pin_memory()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).pin_memory()\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).pin_memory()\n",
    "\n",
    "        # Transfer to GPU asynchronously for speed\n",
    "        return (\n",
    "            states.to(self.device, non_blocking=True),\n",
    "            actions.to(self.device, non_blocking=True),\n",
    "            rewards.to(self.device, non_blocking=True),\n",
    "            next_states.to(self.device, non_blocking=True),\n",
    "            dones.to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9a7fe",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b7af487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episodes=50, batch_size=32, gamma=0.99, lr=1e-4, \n",
    "              epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=5000, \n",
    "              target_update=1000, buffer_size=100000):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    env = gym.make(\"tetris_gymnasium/Tetris\", render_mode=\"ansi\")\n",
    "    env = TetrisObsWrapper(env)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    obs_shape = obs.shape\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    policy_net = DQN(obs_shape, num_actions).to(device)  # Main network\n",
    "    target_net = DQN(obs_shape, num_actions).to(device)  # Target network (copy)\n",
    "    target_net.load_state_dict(policy_net.state_dict())  # Sync weights initially\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = ReplayMemory(buffer_size, device=device)\n",
    "\n",
    "    steps_done = 0\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state)\n",
    "\n",
    "        done = False\n",
    "        rewards_history = []\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action\n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
    "                    np.exp(-1. * steps_done / epsilon_decay)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                s = torch.tensor(state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(s).argmax(1).item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.array(next_state)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            shaped_reward = reward\n",
    "            \n",
    "            if reward == 0:\n",
    "                shaped_reward = -0.01  # penalty for doing nothing\n",
    "\n",
    "            if reward > 0:\n",
    "                shaped_reward = reward * 10  # amplify reward for clearing lines\n",
    "\n",
    "            memory.push(state, action, shaped_reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            # Learn\n",
    "            if len(memory) > batch_size:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "                states, next_states = states.cuda(), next_states.cuda()\n",
    "                actions, rewards, dones = actions.cuda(), rewards.cuda(), dones.cuda()\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = target_net(next_states).max(1)[0]\n",
    "                targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target net\n",
    "            if steps_done % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        # if ep % 50 == 0:\n",
    "        print(f\"Episode {ep}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy_net, rewards_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "911a4115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 11, Epsilon: 0.98\n",
      "Episode 1, Reward: 11, Epsilon: 0.97\n",
      "Episode 2, Reward: 11, Epsilon: 0.96\n",
      "Episode 3, Reward: 11, Epsilon: 0.95\n",
      "Episode 4, Reward: 11, Epsilon: 0.93\n",
      "Episode 5, Reward: 10, Epsilon: 0.92\n",
      "Episode 6, Reward: 8, Epsilon: 0.92\n",
      "Episode 7, Reward: 10, Epsilon: 0.91\n",
      "Episode 8, Reward: 10, Epsilon: 0.90\n",
      "Episode 9, Reward: 14, Epsilon: 0.89\n",
      "Episode 10, Reward: 8, Epsilon: 0.89\n",
      "Episode 11, Reward: 9, Epsilon: 0.88\n",
      "Episode 12, Reward: 10, Epsilon: 0.88\n",
      "Episode 13, Reward: 10, Epsilon: 0.87\n",
      "Episode 14, Reward: 9, Epsilon: 0.86\n",
      "Episode 15, Reward: 8, Epsilon: 0.86\n",
      "Episode 16, Reward: 11, Epsilon: 0.85\n",
      "Episode 17, Reward: 10, Epsilon: 0.84\n",
      "Episode 18, Reward: 7, Epsilon: 0.84\n",
      "Episode 19, Reward: 9, Epsilon: 0.83\n",
      "Episode 20, Reward: 9, Epsilon: 0.83\n",
      "Episode 21, Reward: 8, Epsilon: 0.83\n",
      "Episode 22, Reward: 10, Epsilon: 0.82\n",
      "Episode 23, Reward: 10, Epsilon: 0.81\n",
      "Episode 24, Reward: 9, Epsilon: 0.81\n",
      "Episode 25, Reward: 9, Epsilon: 0.80\n",
      "Episode 26, Reward: 9, Epsilon: 0.80\n",
      "Episode 27, Reward: 9, Epsilon: 0.80\n",
      "Episode 28, Reward: 10, Epsilon: 0.79\n",
      "Episode 29, Reward: 11, Epsilon: 0.79\n",
      "Episode 30, Reward: 8, Epsilon: 0.78\n",
      "Episode 31, Reward: 10, Epsilon: 0.78\n",
      "Episode 32, Reward: 9, Epsilon: 0.78\n",
      "Episode 33, Reward: 11, Epsilon: 0.77\n",
      "Episode 34, Reward: 12, Epsilon: 0.77\n",
      "Episode 35, Reward: 11, Epsilon: 0.76\n",
      "Episode 36, Reward: 9, Epsilon: 0.76\n",
      "Episode 37, Reward: 9, Epsilon: 0.75\n",
      "Episode 38, Reward: 10, Epsilon: 0.75\n",
      "Episode 39, Reward: 8, Epsilon: 0.75\n",
      "Episode 40, Reward: 9, Epsilon: 0.74\n",
      "Episode 41, Reward: 9, Epsilon: 0.74\n",
      "Episode 42, Reward: 10, Epsilon: 0.74\n",
      "Episode 43, Reward: 8, Epsilon: 0.73\n",
      "Episode 44, Reward: 10, Epsilon: 0.73\n",
      "Episode 45, Reward: 11, Epsilon: 0.73\n",
      "Episode 46, Reward: 9, Epsilon: 0.72\n",
      "Episode 47, Reward: 9, Epsilon: 0.72\n",
      "Episode 48, Reward: 9, Epsilon: 0.72\n",
      "Episode 49, Reward: 10, Epsilon: 0.71\n"
     ]
    }
   ],
   "source": [
    "q_net, rewards = train_dqn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
