{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "338885a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import MaxAndSkipObservation\n",
    "from gymnasium.wrappers import FrameStackObservation, ResizeObservation\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from IPython.display import Video\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b09f1d",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a81f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name, obs_type='grayscale')\n",
    "    env = MaxAndSkipObservation(env, skip=8) \n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    env = FrameStackObservation(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71e8e5",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fe7d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95ae5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, input_shape, num_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "#         c, h, w = input_shape  # (4, 84, 84) after transpose\n",
    "#         assert h == 84 and w == 84, \"Input must be 84x84\"\n",
    "\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # compute conv output size dynamically\n",
    "#         def conv2d_size_out(size, kernel_size, stride):\n",
    "#             return (size - kernel_size) // stride + 1\n",
    "#         convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w,8,4),4,2),3,1)\n",
    "#         convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h,8,4),4,2),3,1)\n",
    "#         linear_input_size = convw * convh * 64\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(linear_input_size, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, num_actions)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x / 255.0  # normalize [0,255] → [0,1]\n",
    "#         x = self.conv(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b9489",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05da33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.device = device  # 'cuda' or 'cpu'\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Store raw (numpy or list) to save memory instead of storing tensors directly\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and pin memory for faster GPU transfer\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).pin_memory()\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).pin_memory()\n",
    "        actions = torch.tensor(actions, dtype=torch.long).pin_memory()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).pin_memory()\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).pin_memory()\n",
    "\n",
    "        # Transfer to GPU asynchronously for speed\n",
    "        return (\n",
    "            states.to(self.device, non_blocking=True),\n",
    "            actions.to(self.device, non_blocking=True),\n",
    "            rewards.to(self.device, non_blocking=True),\n",
    "            next_states.to(self.device, non_blocking=True),\n",
    "            dones.to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e941768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity):\n",
    "#         self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "#     def push(self, state, action, reward, next_state, done):\n",
    "#         self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         batch = random.sample(self.buffer, batch_size)\n",
    "#         states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "#         # convert to torch tensors\n",
    "#         states = torch.tensor(states, dtype=torch.float32).permute(0,3,1,2)  # NHWC -> NCHW\n",
    "#         actions = torch.tensor(actions, dtype=torch.long)\n",
    "#         rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "#         next_states = torch.tensor(next_states, dtype=torch.float32).permute(0,3,1,2)\n",
    "#         dones = torch.tensor(dones, dtype=torch.float32)\n",
    "#         return states, actions, rewards, next_states, dones\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b507d1a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episodes=50, batch_size=32, gamma=0.99, lr=1e-4, \n",
    "              epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=200000, \n",
    "              target_update=1000, buffer_size=100000):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    env = make_env(\"ALE/Tetris-v5\")\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    policy_net = DQN(num_actions).to(device)  # Main network\n",
    "    target_net = DQN(num_actions).to(device)  # Target network (copy)\n",
    "    target_net.load_state_dict(policy_net.state_dict())  # Sync weights initially\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = ReplayMemory(buffer_size, device=device)\n",
    "\n",
    "    steps_done = 0\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state)\n",
    "\n",
    "        done = False\n",
    "        rewards_history = []\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action\n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
    "                    np.exp(-1. * steps_done / epsilon_decay)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                s = torch.tensor(state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(s).argmax(1).item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.array(next_state)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            shaped_reward = reward\n",
    "            \n",
    "            if reward == 0:\n",
    "                shaped_reward = -0.01  # penalty for doing nothing\n",
    "\n",
    "            if reward > 0:\n",
    "                shaped_reward = reward * 10  # amplify reward for clearing lines\n",
    "\n",
    "            memory.push(state, action, shaped_reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            # Learn\n",
    "            if len(memory) > batch_size:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "                states, next_states = states.cuda(), next_states.cuda()\n",
    "                actions, rewards, dones = actions.cuda(), rewards.cuda(), dones.cuda()\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = target_net(next_states).max(1)[0]\n",
    "                targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target net\n",
    "            if steps_done % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if ep % 50 == 0:\n",
    "            print(f\"Episode {ep}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy_net, rewards_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb1af07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 0.0, Epsilon: 1.00\n"
     ]
    }
   ],
   "source": [
    "q_net, rewards = train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51bb64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torch\n",
    "\n",
    "def evaluate_and_record(q_net, env_name=\"ALE/Tetris-v5\", video_path=\"tetris_dqn.mp4\", episodes=3):\n",
    "    env = make_env(env_name)\n",
    "    raw_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    writer = imageio.get_writer(video_path, fps=30)\n",
    "\n",
    "    device = next(q_net.parameters()).device  # auto-detect CUDA or CPU\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        raw_env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)  # MOVED TO GPU/CPU MATCH\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(q_net(state_tensor)).item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            raw_env.step(action)\n",
    "\n",
    "            frame = raw_env.render()\n",
    "            writer.append_data(frame)\n",
    "\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "    writer.close()\n",
    "    env.close()\n",
    "    raw_env.close()\n",
    "    print(f\"✅ Video saved to {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1109cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Video saved to tetris_dqn.mp4\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_record(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49cff7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 4, 8, 8], expected input[1, 210, 160, 3] to have 4 channels, but got 210 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m state_tensor = torch.FloatTensor(state)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     action = torch.argmax(\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m).item()\n\u001b[32m     12\u001b[39m state, _, terminated, truncated, _ = env.step(action)\n\u001b[32m     13\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Neil\\miniconda3\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Neil\\miniconda3\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     12\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x))\n\u001b[32m     13\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Neil\\miniconda3\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Neil\\miniconda3\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Neil\\miniconda3\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Neil\\miniconda3\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [32, 4, 8, 8], expected input[1, 210, 160, 3] to have 4 channels, but got 210 channels instead"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"ALE/Tetris-v5\", render_mode=\"rgb_array\")\n",
    "# video_path = \"tetris_dqn.mp4\"\n",
    "# writer = imageio.get_writer(video_path, fps=30)\n",
    "\n",
    "# for ep in range(3):\n",
    "#     state, _ = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         state_tensor = torch.FloatTensor(state)\n",
    "#         with torch.no_grad():\n",
    "#             action = torch.argmax(q_net(state_tensor)).item()\n",
    "#         state, _, terminated, truncated, _ = env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         frame = env.render()\n",
    "#         writer.append_data(frame)\n",
    "\n",
    "# writer.close()\n",
    "# env.close()\n",
    "# print(f\"Video saved to {video_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
