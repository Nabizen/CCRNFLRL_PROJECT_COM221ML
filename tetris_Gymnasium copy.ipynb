{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ef7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from IPython.display import Video\n",
    "from tetris_gymnasium.envs.tetris import Tetris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b96a3",
   "metadata": {},
   "source": [
    "### **Wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ea08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class TetrisObsWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Define final observation space (4 channels, 24x18)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(4, 24, 18),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        board = obs[\"board\"].astype(np.float32) / 9.0\n",
    "        mask = obs[\"active_tetromino_mask\"].astype(np.float32)\n",
    "        holder = obs[\"holder\"].astype(np.float32) / 9.0\n",
    "        queue = obs[\"queue\"].astype(np.float32) / 9.0\n",
    "\n",
    "        # pad holder to (24, 18)\n",
    "        holder_padded = np.zeros((24, 18), dtype=np.float32)\n",
    "        holder_padded[:holder.shape[0], :holder.shape[1]] = holder\n",
    "\n",
    "        # pad or crop queue safely to fit (24, 18)\n",
    "        queue_padded = np.zeros((24, 18), dtype=np.float32)\n",
    "        h, w = queue.shape\n",
    "        queue_padded[:min(4, h), :min(18, w)] = queue[:min(4, h), :min(18, w)]\n",
    "\n",
    "        # stack all channels\n",
    "        stacked = np.stack([board, mask, holder_padded, queue_padded], axis=0)\n",
    "        return stacked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4bdee",
   "metadata": {},
   "source": [
    "### **Neural Network (DQN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6889c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        C, H, W = obs_shape  # e.g. (4, 24, 18)\n",
    "\n",
    "        # Safer kernel/stride combo for small inputs\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(C, 32, kernel_size=3, stride=1, padding=1),  # keeps 24x18\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # halves size\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),  # halves again\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Automatically compute flattened conv output size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, C, H, W)\n",
    "            conv_out = self.conv(dummy)\n",
    "            conv_out_size = conv_out.view(1, -1).size(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57040f2e",
   "metadata": {},
   "source": [
    "### **ReplayBuffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "625b377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.device = device  # 'cuda' or 'cpu'\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Store raw (numpy or list) to save memory instead of storing tensors directly\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and pin memory for faster GPU transfer\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).pin_memory()\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).pin_memory()\n",
    "        actions = torch.tensor(actions, dtype=torch.long).pin_memory()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).pin_memory()\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).pin_memory()\n",
    "\n",
    "        # Transfer to GPU asynchronously for speed\n",
    "        return (\n",
    "            states.to(self.device, non_blocking=True),\n",
    "            actions.to(self.device, non_blocking=True),\n",
    "            rewards.to(self.device, non_blocking=True),\n",
    "            next_states.to(self.device, non_blocking=True),\n",
    "            dones.to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9a7fe",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd43e90",
   "metadata": {},
   "source": [
    "#### *DQN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7af487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episodes, batch_size, gamma, lr, \n",
    "              epsilon_start, epsilon_end, epsilon_decay, \n",
    "              target_update, buffer_size):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    env = gym.make(\"tetris_gymnasium/Tetris\", render_mode=\"ansi\")\n",
    "    env = TetrisObsWrapper(env)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    obs_shape = obs.shape\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    policy_net = DQN(obs_shape, num_actions).to(device)\n",
    "    target_net = DQN(obs_shape, num_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = ReplayMemory(buffer_size, device=device)\n",
    "\n",
    "    steps_done = 0\n",
    "    epsilon = epsilon_start\n",
    "    rewards_history = []\n",
    "    lines_history = []  # ðŸŸ© new: track lines cleared per episode\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        lines_cleared = 0   # ðŸŸ© new: counter for lines cleared this episode\n",
    "\n",
    "        while not done:\n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
    "                    np.exp(-1. * steps_done / epsilon_decay)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                s = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(s).argmax(1).item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = np.array(next_state)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # ðŸŸ© count lines cleared from environment info if available\n",
    "            #   (most Tetris envs include this in `info`, adjust if your env differs)\n",
    "            if \"lines_cleared\" in info:\n",
    "                lines_cleared += info[\"lines_cleared\"]\n",
    "\n",
    "            # Reward shaping\n",
    "            shaped_reward = reward\n",
    "            if 1 > reward >= 0:\n",
    "                shaped_reward = 0.1\n",
    "            elif reward >= 1:\n",
    "                shaped_reward = 1 * (reward ** 2) # (10 * [1, 4, 9, and 16])\n",
    "\n",
    "            memory.push(state, action, shaped_reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            # Learn\n",
    "            if len(memory) > batch_size:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "                states, next_states = states.to(device), next_states.to(device)\n",
    "                actions, rewards, dones = actions.to(device), rewards.to(device), dones.to(device)\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = target_net(next_states).max(1)[0]\n",
    "                targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Target network update\n",
    "            if steps_done % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        lines_history.append(lines_cleared)\n",
    "\n",
    "        print(f\"Episode {ep}, Reward: {total_reward:.3f}, Lines Cleared: {lines_cleared}\")\n",
    "\n",
    "        # ðŸŸ© Display progress every 500 episodes\n",
    "        if (ep + 1) % 500 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-500:])\n",
    "            avg_lines = np.mean(lines_history[-500:])\n",
    "            print(f\"[Episode {ep+1:5d}] Avg Reward (last 500): {avg_reward:.3f} | \"\n",
    "                  f\"Avg Lines: {avg_lines:.2f} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy_net, rewards_history #, lines_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the path of the reward mapping file\n",
    "\n",
    "import tetris_gymnasium.mappings.rewards as rewards\n",
    "print(rewards.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tetris_gymnasium.mappings.rewards import RewardsMapping\n",
    "rewards_mapping = RewardsMapping()\n",
    "print(rewards_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "params_default = {\n",
    "    \"episodes\": 5000,          # short training duration\n",
    "    \"batch_size\": 512,          # small batch, faster but noisier updates\n",
    "    \"gamma\": 0.99,             # typical default discount factor\n",
    "    \"lr\": 0.001,              # faster learning rate (less stable)\n",
    "    \"epsilon_start\": 1.0,      # full exploration at start\n",
    "    \"epsilon_end\": 0.001,        # mild exploration at the end\n",
    "    \"epsilon_decay\": 2000,     # fast decay (less exploration overall)\n",
    "    \"target_update\": 1000,     # frequent target refreshes\n",
    "    \"buffer_size\": 30000      # smaller replay buffer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net, rewards = train_dqn(**params_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Training Rewards over Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b2981",
   "metadata": {},
   "source": [
    "#### Video Recording (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72754f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "env = gym.make(\"tetris_gymnasium/Tetris\", render_mode=\"rgb_array\")  # use rgb_array for frames\n",
    "# env = ActionFilterWrapper(env, allowed_actions=[0, 1, 2, 3, 4, 5]) # (remove swap=6 and no_op=7)  \n",
    "env = TetrisObsWrapper(env)\n",
    "\n",
    "# Folder for saving videos\n",
    "video_dir = \"tetris_videos\"\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Pattern: tetris_dqn_<number>.mp4\n",
    "existing = [\n",
    "    f for f in os.listdir(video_dir)\n",
    "    if re.match(r\"tetris_dqn_default-params_(\\d+)\\.mp4\", f)\n",
    "]\n",
    "\n",
    "if existing:\n",
    "    # Extract numbers from filenames and find the largest one\n",
    "    last_num = max(int(re.search(r\"tetris_dqn_default-params_(\\d+)\\.mp4\", f).group(1)) for f in existing)\n",
    "    next_num = last_num + 1\n",
    "else:\n",
    "    next_num = 1\n",
    "\n",
    "# Build next video filename\n",
    "video_path = os.path.join(video_dir, f\"tetris_dqn_default-params_{next_num}.mp4\")\n",
    "writer = imageio.get_writer(video_path, fps=30)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_net.eval()\n",
    "\n",
    "for ep in range(10):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # convert state (numpy) to torch tensor and add batch dim\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = q_net(state_tensor).argmax(1).item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # render as rgb_array and write to video\n",
    "        frame = env.render()\n",
    "        writer.append_data(frame)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {ep + 1} finished with reward {total_reward:.3f}\")\n",
    "\n",
    "writer.close()\n",
    "env.close()\n",
    "print(f\"ðŸŽ¬ Video saved to {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8903526",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"tetris_videos/tetris_dqn_default-params_17.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bb0ca",
   "metadata": {},
   "source": [
    "### *PPO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fceba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit Action Space to Relevant Actions Only\n",
    "\n",
    "import gymnasium as gym\n",
    "class ActionFilterWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, allowed_actions):\n",
    "        super().__init__(env)\n",
    "        self.allowed_actions = list(allowed_actions)\n",
    "        self.action_map = {i: a for i, a in enumerate(self.allowed_actions)}\n",
    "        self.action_space = gym.spaces.Discrete(len(self.allowed_actions))\n",
    "\n",
    "    def action(self, act):\n",
    "        return int(self.action_map[int(act)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9600e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose Observation Wrapper\n",
    "\n",
    "class TransposeObsWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_space = env.observation_space\n",
    "        if not isinstance(obs_space, gym.spaces.Box):\n",
    "            raise ValueError(\"TransposeObsWrapper expects Box observation space\")\n",
    "        C, H, W = obs_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=obs_space.low.min(),\n",
    "            high=obs_space.high.max(),\n",
    "            shape=(H, W, C),\n",
    "            dtype=obs_space.dtype\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return np.transpose(obs, (1, 2, 0))  # (H, W, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5656a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from tetris_gymnasium.wrappers.observation import FeatureVectorObservation\n",
    "import os\n",
    "\n",
    "# # Create log directory\n",
    "# log_dir = \"./logs/\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def make_tetris_env(render_mode=None):\n",
    "    def _init():\n",
    "        env = gym.make(\"tetris_gymnasium/Tetris\", render_mode=render_mode)\n",
    "        env = FeatureVectorObservation(env)\n",
    "        # env = TetrisObsWrapper(env)\n",
    "        # env = ActionFilterWrapper(env)\n",
    "        # env = TransposeObsWrapper(env)\n",
    "        # env = Monitor(env, log_dir)  # ðŸ‘ˆ Logs episode reward, length, etc.\n",
    "        return env\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "456497fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(8)\n",
      "Action space n: 8\n"
     ]
    }
   ],
   "source": [
    "env_fn = make_tetris_env()\n",
    "env = env_fn()  # actually create one instance\n",
    "\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Action space n:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3216da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RewardsMapping(alife=0.001, clear_line=1, game_over=-2, invalid_action=-0.1)\n"
     ]
    }
   ],
   "source": [
    "from tetris_gymnasium.mappings.rewards import RewardsMapping\n",
    "rewards_mapping = RewardsMapping()\n",
    "print(rewards_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05bccfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class SmallTetrisCNN(nn.Module):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        n_input_channels = observation_space.shape[2]  # (H, W, C)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> (12x9)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> (6x4)\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Determine flatten size automatically\n",
    "        with th.no_grad():\n",
    "            sample = th.as_tensor(observation_space.sample()[None]).float()\n",
    "            sample = sample.permute(0, 3, 1, 2)  # (B,C,H,W)\n",
    "            n_flatten = self.cnn(sample).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # REQUIRED by SB3\n",
    "        self.features_dim = features_dim\n",
    "\n",
    "    def forward(self, obs: th.Tensor) -> th.Tensor:\n",
    "        obs = obs.permute(0, 3, 1, 2)  # (B,H,W,C) â†’ (B,C,H,W)\n",
    "        return self.linear(self.cnn(obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a4b539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Train PPO with Stable Baselines3\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "# parameters\n",
    "n_envs = 4\n",
    "total_timesteps = 1_000_000   # 1M steps\n",
    "learning_rate = 1e-3          # 0.001\n",
    "n_steps = 2048                # per-env rollout length (2k)\n",
    "batch_size = 64\n",
    "n_epochs = 20\n",
    "seed = 0\n",
    "\n",
    "# Build vectorized env (SubprocVecEnv for better parallelization)\n",
    "env_fns = [make_tetris_env() for _ in range(n_envs)]\n",
    "vec_env = DummyVecEnv(env_fns) if n_envs > 1 else DummyVecEnv(env_fns)\n",
    "\n",
    "# PPO policy configuration\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=SmallTetrisCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    "    net_arch=[dict(pi=[128, 64], vf=[128, 64])]\n",
    ")\n",
    "\n",
    "# print(\"Observation space:\", vec_env.observation_space)\n",
    "\n",
    "# Create and train PPO model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    learning_rate=learning_rate,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    "    # policy_kwargs=policy_kwargs,\n",
    "    seed=seed,\n",
    "    verbose=1,  # for detailed logging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40468ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 727  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019967353 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | -0.69       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.00746    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 0.0397      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 229       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 106       |\n",
      "|    total_timesteps      | 24576     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0208431 |\n",
      "|    clip_fraction        | 0.282     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.02     |\n",
      "|    explained_variance   | 0.789     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 0.016     |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0262   |\n",
      "|    value_loss           | 0.0206    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021353282 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2          |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024039596 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0341     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.00945     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 175         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024493553 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0419     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.0616      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027286282 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.00819     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027562588 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.00598     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 452        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03050147 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.82      |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    value_loss           | 0.00551    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 510         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031959094 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0533     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.00429     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 568         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029962864 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.00458     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 625        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03179497 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.75      |\n",
      "|    explained_variance   | 0.938      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    value_loss           | 0.00484    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 684        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03342057 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.74      |\n",
      "|    explained_variance   | 0.592      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0934     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    value_loss           | 0.0902     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 741        |\n",
      "|    total_timesteps      | 114688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03578919 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.71      |\n",
      "|    explained_variance   | 0.866      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    value_loss           | 0.00777    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 799         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034705076 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.00544     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 857         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035957567 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 916         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035911627 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.00447     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 961         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039238937 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 997        |\n",
      "|    total_timesteps      | 155648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04725111 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | 0.405      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.00881   |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 0.0307     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 1034        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037841547 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.527       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 0.0933      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1070        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038708556 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1105        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041204438 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.19        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1141        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042051256 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0355     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.00899     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1178        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040408425 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.00505     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1213        |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043204222 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.00526    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.00483     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 1250        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046222992 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.039      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1286        |\n",
      "|    total_timesteps      | 221184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041176826 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 0.0984      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1318        |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046347477 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.00869    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 176         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 1348        |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042010233 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00877     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.07        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1380        |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044618327 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00161     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 179         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1414        |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043776896 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0422     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.00713     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 180         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1454        |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043443542 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.159       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.0732      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1483        |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043616243 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00747     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1513        |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043153528 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.012       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 185        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 1544       |\n",
      "|    total_timesteps      | 286720     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04636838 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.102      |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 186        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 1578       |\n",
      "|    total_timesteps      | 294912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04936585 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | 0.693      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.158      |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    value_loss           | 0.0797     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 1607       |\n",
      "|    total_timesteps      | 303104     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04814798 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.19      |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0609    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 0.00837    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 1639        |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047541507 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 191        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 1672       |\n",
      "|    total_timesteps      | 319488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04894686 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.00418   |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    value_loss           | 0.013      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 1705        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053099476 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0221     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.0535      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 1739        |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056494102 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.00671    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 1772        |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051396433 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0285      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.0252      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 1806        |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053548336 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 1839        |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054246843 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.436       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1872        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058862444 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1918        |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056028925 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.0157      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 195       |\n",
      "|    iterations           | 47        |\n",
      "|    time_elapsed         | 1965      |\n",
      "|    total_timesteps      | 385024    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0538209 |\n",
      "|    clip_fraction        | 0.358     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.12     |\n",
      "|    explained_variance   | 0.936     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | -0.0518   |\n",
      "|    n_updates            | 920       |\n",
      "|    policy_gradient_loss | -0.0179   |\n",
      "|    value_loss           | 0.00869   |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 2022        |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053198297 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.039      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.00615     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 193        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 2079       |\n",
      "|    total_timesteps      | 401408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05439843 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0156    |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 0.00507    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 191        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 2136       |\n",
      "|    total_timesteps      | 409600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05218205 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.00123   |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 0.0047     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 2193        |\n",
      "|    total_timesteps      | 417792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056511108 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.00733     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 2231        |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058150396 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.00602     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 2274        |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054887675 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0216     |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.0941      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 2306        |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052582465 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 0.0115      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 2336        |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074963436 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 2365        |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059324402 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0207     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.0171      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 2395        |\n",
      "|    total_timesteps      | 466944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056331336 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0374      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 195       |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 2425      |\n",
      "|    total_timesteps      | 475136    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0591079 |\n",
      "|    clip_fraction        | 0.384     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.12     |\n",
      "|    explained_variance   | 0.565     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 0.167     |\n",
      "|    n_updates            | 1140      |\n",
      "|    policy_gradient_loss | -0.0111   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 196        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 2454       |\n",
      "|    total_timesteps      | 483328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06246555 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.896      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    value_loss           | 0.011      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 197        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 2484       |\n",
      "|    total_timesteps      | 491520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06254917 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.648      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0389    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 198        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 2514       |\n",
      "|    total_timesteps      | 499712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06153114 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.472      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0671     |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    value_loss           | 0.187      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 199        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 2542       |\n",
      "|    total_timesteps      | 507904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06000439 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.567      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0185    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 200        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 2574       |\n",
      "|    total_timesteps      | 516096     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06231356 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.591      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0155     |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 0.105      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 201        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 2601       |\n",
      "|    total_timesteps      | 524288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06040227 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    value_loss           | 0.084      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 202        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 2629       |\n",
      "|    total_timesteps      | 532480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06429526 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.475      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0797     |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 202        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 2666       |\n",
      "|    total_timesteps      | 540672     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06918743 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.577      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    value_loss           | 0.0651     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 2703        |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061802983 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.983      |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 2733        |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056084458 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00368     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 2761        |\n",
      "|    total_timesteps      | 565248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060056575 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0506     |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 2789        |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053201362 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 2814        |\n",
      "|    total_timesteps      | 581632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062949136 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00472     |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.0813      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 2840        |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060682338 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 2867       |\n",
      "|    total_timesteps      | 598016     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05620162 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.899     |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    value_loss           | 0.011      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 2894       |\n",
      "|    total_timesteps      | 606208     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06165435 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.883     |\n",
      "|    explained_variance   | 0.682      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0187    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    value_loss           | 0.0939     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 2920        |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062287427 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0348      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 2948        |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058915187 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 0.0129      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 2975        |\n",
      "|    total_timesteps      | 630784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061417177 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.000529    |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.00776     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 3003       |\n",
      "|    total_timesteps      | 638976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06087723 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.935     |\n",
      "|    explained_variance   | 0.698      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    value_loss           | 0.092      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 3047       |\n",
      "|    total_timesteps      | 647168     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07140343 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.937     |\n",
      "|    explained_variance   | 0.482      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0354    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 0.192      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 3094       |\n",
      "|    total_timesteps      | 655360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06541561 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.938     |\n",
      "|    explained_variance   | 0.85       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0549    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    value_loss           | 0.02       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 3138       |\n",
      "|    total_timesteps      | 663552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06462008 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.922     |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 0.0102     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 3182       |\n",
      "|    total_timesteps      | 671744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07202395 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.945     |\n",
      "|    explained_variance   | 0.692      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0612    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 3226       |\n",
      "|    total_timesteps      | 679936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06561506 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.957     |\n",
      "|    explained_variance   | 0.642      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.00602   |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 3270        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060185708 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0602     |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 0.0105      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 3316        |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063280776 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0373     |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.0991      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 3362       |\n",
      "|    total_timesteps      | 704512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06432017 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.972     |\n",
      "|    explained_variance   | 0.626      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 3410       |\n",
      "|    total_timesteps      | 712704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06802042 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.976     |\n",
      "|    explained_variance   | 0.672      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0464    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    value_loss           | 0.0838     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 3459       |\n",
      "|    total_timesteps      | 720896     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07449478 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.988     |\n",
      "|    explained_variance   | 0.301      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0372     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    value_loss           | 0.295      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 3505       |\n",
      "|    total_timesteps      | 729088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07253727 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.964     |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    value_loss           | 0.191      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 3550       |\n",
      "|    total_timesteps      | 737280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07280037 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.949     |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.00364    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    value_loss           | 0.0877     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 3597       |\n",
      "|    total_timesteps      | 745472     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06454307 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.948     |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0528    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    value_loss           | 0.0121     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 206        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 3645       |\n",
      "|    total_timesteps      | 753664     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07030892 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.924     |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0445    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    value_loss           | 0.0106     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 3680        |\n",
      "|    total_timesteps      | 761856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060424604 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.00892     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 3707       |\n",
      "|    total_timesteps      | 770048     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06902037 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.925     |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    value_loss           | 0.101      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 3734       |\n",
      "|    total_timesteps      | 778240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06749052 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.922     |\n",
      "|    explained_variance   | 0.696      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0227    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    value_loss           | 0.0833     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 3761        |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070898406 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0819     |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 3791        |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061584007 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.0177      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 3818       |\n",
      "|    total_timesteps      | 802816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06745079 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | 0.948      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    value_loss           | 0.00816    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 3846       |\n",
      "|    total_timesteps      | 811008     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07697175 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.983     |\n",
      "|    explained_variance   | 0.595      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0256    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    value_loss           | 0.101      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 3875        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060066756 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0153     |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 0.0131      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 101        |\n",
      "|    time_elapsed         | 3904       |\n",
      "|    total_timesteps      | 827392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06803911 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.955     |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.00826   |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    value_loss           | 0.00919    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 102        |\n",
      "|    time_elapsed         | 3939       |\n",
      "|    total_timesteps      | 835584     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06859645 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.942     |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0251     |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    value_loss           | 0.00675    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 103        |\n",
      "|    time_elapsed         | 3965       |\n",
      "|    total_timesteps      | 843776     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07081337 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.967     |\n",
      "|    explained_variance   | 0.481      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    value_loss           | 0.186      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 104        |\n",
      "|    time_elapsed         | 4007       |\n",
      "|    total_timesteps      | 851968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07326841 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.033     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    value_loss           | 0.0121     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 105        |\n",
      "|    time_elapsed         | 4037       |\n",
      "|    total_timesteps      | 860160     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06491311 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | 0.612      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 106        |\n",
      "|    time_elapsed         | 4066       |\n",
      "|    total_timesteps      | 868352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06634435 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.996     |\n",
      "|    explained_variance   | 0.863      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0256     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    value_loss           | 0.0119     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 107        |\n",
      "|    time_elapsed         | 4094       |\n",
      "|    total_timesteps      | 876544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07253425 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.997     |\n",
      "|    explained_variance   | 0.59       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.681      |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 108        |\n",
      "|    time_elapsed         | 4124       |\n",
      "|    total_timesteps      | 884736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07083711 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.988     |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0155    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    value_loss           | 0.0105     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 109         |\n",
      "|    time_elapsed         | 4151        |\n",
      "|    total_timesteps      | 892928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070789024 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.987      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00677     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.0103      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 110        |\n",
      "|    time_elapsed         | 4178       |\n",
      "|    total_timesteps      | 901120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07784304 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.952     |\n",
      "|    explained_variance   | 0.935      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.013     |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    value_loss           | 0.0104     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 4208        |\n",
      "|    total_timesteps      | 909312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074098304 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.159       |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.0868      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 112        |\n",
      "|    time_elapsed         | 4261       |\n",
      "|    total_timesteps      | 917504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06995663 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.953     |\n",
      "|    explained_variance   | 0.614      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0522    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 113        |\n",
      "|    time_elapsed         | 4314       |\n",
      "|    total_timesteps      | 925696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07780623 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.929     |\n",
      "|    explained_variance   | 0.613      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.0302     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 0.0693     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 114        |\n",
      "|    time_elapsed         | 4367       |\n",
      "|    total_timesteps      | 933888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06494237 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.954     |\n",
      "|    explained_variance   | 0.618      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.02       |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    value_loss           | 0.0774     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 115        |\n",
      "|    time_elapsed         | 4421       |\n",
      "|    total_timesteps      | 942080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07097812 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.92      |\n",
      "|    explained_variance   | 0.573      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0222    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.00773   |\n",
      "|    value_loss           | 0.0868     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 4475        |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068267465 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00391     |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 4529        |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082494006 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0672     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.00871    |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 118         |\n",
      "|    time_elapsed         | 4583        |\n",
      "|    total_timesteps      | 966656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079622366 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.914      |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.384       |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    value_loss           | 0.323       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 4636        |\n",
      "|    total_timesteps      | 974848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079984576 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.889      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.0199      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 120        |\n",
      "|    time_elapsed         | 4689       |\n",
      "|    total_timesteps      | 983040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06940913 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.878     |\n",
      "|    explained_variance   | 0.61       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    value_loss           | 0.0722     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 121        |\n",
      "|    time_elapsed         | 4742       |\n",
      "|    total_timesteps      | 991232     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06589985 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.863     |\n",
      "|    explained_variance   | 0.776      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 0.0198     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 122        |\n",
      "|    time_elapsed         | 4795       |\n",
      "|    total_timesteps      | 999424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07260098 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.846     |\n",
      "|    explained_variance   | 0.449      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0435    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.00517   |\n",
      "|    value_loss           | 0.187      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 123        |\n",
      "|    time_elapsed         | 4849       |\n",
      "|    total_timesteps      | 1007616    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07365371 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.865     |\n",
      "|    explained_variance   | 0.637      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Save\n",
    "model.save(\"ppo_tetris_model_v1\")\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa7a778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Eval mean reward: -1.58 Â± 1.96\n",
      "Episode reward: -1.98\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell: eval\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "eval_env = make_tetris_env()( )  # single env instance\n",
    "model = PPO.load(\"ppo_tetris_model_v1\", env=eval_env)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True)\n",
    "print(f\"Eval mean reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "\n",
    "# If you want to render a single episode and inspect:\n",
    "obs, _ = eval_env.reset()\n",
    "done = False\n",
    "ep_reward = 0\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(int(action))\n",
    "    ep_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(\"Episode reward:\", ep_reward)\n",
    "eval_env.close()\n",
    "\n",
    "\n",
    "# visualize mean and std returned by evaluate_policy\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(0, mean_reward, yerr=std_reward, capsize=10, color='C0', alpha=0.8)\n",
    "plt.xlim(-0.6, 0.6)\n",
    "plt.xticks([])  # no x-axis labels for a single-bar plot\n",
    "plt.ylabel('Reward')\n",
    "plt.title('PPO Evaluation over 100 episodes\\nMean Reward Â± Std')\n",
    "plt.text(0, mean_reward + np.sign(mean_reward + 1e-8) * (std_reward + 0.05),\n",
    "         f'{mean_reward:.2f} Â± {std_reward:.2f}',\n",
    "         ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f62ae31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (340, 240) to (352, 240) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Video saved to ppo_tetris_play.mp4\n",
      "Total reward accumulated: -13.87\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import imageio\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 1. Load your trained model\n",
    "model = PPO.load(\"ppo_tetris_model_v1\")\n",
    "\n",
    "# 2. Recreate environment with render_mode='rgb_array'\n",
    "env = make_tetris_env(render_mode='rgb_array')()  # if your make_tetris_env supports args\n",
    "obs, _ = env.reset()\n",
    "\n",
    "frames = []\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(2000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    # now render() returns the RGB array directly\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "# 4. Save video\n",
    "output_path = \"ppo_tetris_play.mp4\"\n",
    "imageio.mimsave(output_path, frames, fps=30)\n",
    "\n",
    "print(f\"âœ… Video saved to {output_path}\")\n",
    "print(f\"Total reward accumulated: {total_reward:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {\n",
    "    \"episodes\": 2000,          # short training duration\n",
    "    \"batch_size\": 64,          # small batch, faster but noisier updates\n",
    "    \"gamma\": 0.99,             # typical default discount factor\n",
    "    \"lr\": 0.001,              # faster learning rate (less stable)\n",
    "    \"epsilon_start\": 1.0,      # full exploration at start\n",
    "    \"epsilon_end\": 0.05,        # mild exploration at the end\n",
    "    \"epsilon_decay\": 25000,     # fast decay (less exploration overall)\n",
    "    \"target_update\": 1000,     # frequent target refreshes\n",
    "    \"buffer_size\": 3000      # smaller replay buffer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGZ!!\n",
    " \n",
    "q_net, rewards = train_dqn(**tuned_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Training Rewards over Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba994d",
   "metadata": {},
   "source": [
    "#### Video Recording (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab57f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "env = gym.make(\"tetris_gymnasium/Tetris\", render_mode=\"rgb_array\")  # use rgb_array for frames\n",
    "env = TetrisObsWrapper(env)\n",
    "\n",
    "# Folder for saving videos\n",
    "video_dir = \"tetris_videos\"\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Pattern: tetris_dqn_<number>.mp4\n",
    "existing = [\n",
    "    f for f in os.listdir(video_dir)\n",
    "    if re.match(r\"tetris_dqn_tuned-params_(\\d+)\\.mp4\", f)\n",
    "]\n",
    "\n",
    "if existing:\n",
    "    # Extract numbers from filenames and find the largest one\n",
    "    last_num = max(int(re.search(r\"tetris_dqn_tuned-params_(\\d+)\\.mp4\", f).group(1)) for f in existing)\n",
    "    next_num = last_num + 1\n",
    "else:\n",
    "    next_num = 1\n",
    "\n",
    "# Build next video filename\n",
    "video_path = os.path.join(video_dir, f\"tetris_dqn_tuned-params_{next_num}.mp4\")\n",
    "writer = imageio.get_writer(video_path, fps=30)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_net.eval()\n",
    "\n",
    "for ep in range(10):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # convert state (numpy) to torch tensor and add batch dim\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = q_net(state_tensor).argmax(1).item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # render as rgb_array and write to video\n",
    "        frame = env.render()\n",
    "        writer.append_data(frame)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {ep + 1} finished with reward {total_reward:.3f}\")\n",
    "\n",
    "writer.close()\n",
    "env.close()\n",
    "print(f\"ðŸŽ¬ Video saved to {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"tetris_videos/tetris_dqn_tuned-params_2.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d5104",
   "metadata": {},
   "source": [
    "### **Actor Critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b9146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================\n",
    "# # ðŸ“Š Evaluate trained Tetris DQN agent\n",
    "# # =============================\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import gymnasium as gym\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # --- define evaluation function ---\n",
    "# def evaluate_agent(policy_net,\n",
    "#                    env_fn,\n",
    "#                    n_episodes=100,\n",
    "#                    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                    render=False,\n",
    "#                    render_mode='ansi',\n",
    "#                    collect_shaped=True):\n",
    "\n",
    "#     policy_net.eval()\n",
    "#     stats = defaultdict(list)\n",
    "\n",
    "#     for ep in range(n_episodes):\n",
    "#         env = env_fn()\n",
    "#         obs, _ = env.reset()\n",
    "#         state = np.array(obs, dtype=np.float32)\n",
    "#         done = False\n",
    "#         total_reward = 0.0\n",
    "#         total_shaped = 0.0 if collect_shaped else None\n",
    "#         steps = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             while not done:\n",
    "#                 s = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "#                 q_values = policy_net(s)\n",
    "#                 action = q_values.argmax(1).item()\n",
    "\n",
    "#                 next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "#                 next_state = np.array(next_obs, dtype=np.float32)\n",
    "#                 done = terminated or truncated\n",
    "\n",
    "#                 total_reward += reward\n",
    "#                 if collect_shaped:\n",
    "#                     shaped = -0.01 if reward == 0 else reward * 10\n",
    "#                     total_shaped += shaped\n",
    "\n",
    "#                 state = next_state\n",
    "#                 steps += 1\n",
    "\n",
    "#         env.close()\n",
    "\n",
    "#         stats['episode_reward_raw'].append(total_reward)\n",
    "#         if collect_shaped:\n",
    "#             stats['episode_reward_shaped'].append(total_shaped)\n",
    "#         stats['episode_steps'].append(steps)\n",
    "\n",
    "#         print(f\"Episode {ep+1}/{n_episodes} | Lines Cleared: {total_reward:.0f} | Steps: {steps}\")\n",
    "\n",
    "#     # --- summary metrics ---\n",
    "#     metrics = {\n",
    "#         'mean_reward_raw': np.mean(stats['episode_reward_raw']),\n",
    "#         'std_reward_raw': np.std(stats['episode_reward_raw']),\n",
    "#         'max_reward_raw': np.max(stats['episode_reward_raw']),\n",
    "#         'mean_steps': np.mean(stats['episode_steps'])\n",
    "#     }\n",
    "#     if collect_shaped:\n",
    "#         metrics['mean_reward_shaped'] = np.mean(stats['episode_reward_shaped'])\n",
    "\n",
    "#     print(\"\\n=== Evaluation Summary ===\")\n",
    "#     for k, v in metrics.items():\n",
    "#         print(f\"{k:20s}: {v:.2f}\")\n",
    "\n",
    "#     # --- plot ---\n",
    "#     plt.figure(figsize=(10,4))\n",
    "#     plt.plot(stats['episode_reward_raw'], label='Raw (Lines Cleared)')\n",
    "#     if collect_shaped:\n",
    "#         plt.plot(stats['episode_reward_shaped'], label='Shaped Reward')\n",
    "#     plt.xlabel('Episode')\n",
    "#     plt.ylabel('Reward')\n",
    "#     plt.title('DQN Evaluation over 100 Episodes')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "#     return metrics, stats\n",
    "\n",
    "\n",
    "# # --- run evaluation ---\n",
    "# env_ctor = lambda: TetrisObsWrapper(gym.make(\"tetris_gymnasium/Tetris\", render_mode=\"ansi\"))\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # replace `q_net` with whatever your trained model variable name is\n",
    "# metrics, stats = evaluate_agent(q_net, env_ctor, n_episodes=1000, device=device, collect_shaped=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_finals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
